{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOj/YccKj9C4ge9kP79nTjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klordo/nlp_homeworks/blob/hw7/nlp_hw7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Библиотеки"
      ],
      "metadata": {
        "id": "rh_oofQSpfYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "IUsAJwE5Bgk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizerFast, DistilBertTokenizer\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import pipeline\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW"
      ],
      "metadata": {
        "id": "uU3Ol0GVEKrT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPRReader, DPRReaderTokenizer"
      ],
      "metadata": {
        "id": "BIqEaRJhERH8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "rRJTFisHENKw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка датасета"
      ],
      "metadata": {
        "id": "-z-6l12Cpcoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"sberquad\")"
      ],
      "metadata": {
        "id": "SYEB69WiJUgC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcuoFhUxJetV",
        "outputId": "115dac83-4db5-4ace-9b85-378671f24d8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 45328\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5036\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 23936\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Одна запись c плохим ответом, ее решил просто убрать"
      ],
      "metadata": {
        "id": "HqwUMrvLgw0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].filter(lambda x: x['id'] == 61603)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFZdb_SYeFIN",
        "outputId": "733fa038-2def-4441-90a6-eba6c9422dd7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 61603,\n",
              " 'title': 'SberChallenge',\n",
              " 'context': 'Четвёртый альбом Issues (1999) группы Korn также попал на вершину Billboard 200.[118][153] За один месяц он стал трижды платиновым.[154] За первую неделю он разошёлся тиражом 573,000 копий,[153] а первый сингл Falling Away from Me достиг 99 позиции в Billboard Hot 100.[155] Немногим раньше, до выхода альбома, Korn появляются в эпизоде Отличная загадка группы Korn о пиратском призраке мультсериала Южный парк, в котором состоялась премьера сингла Falling Away from Me .[156][157] В конце 1990-х и начале 2000-х различные ню-метал-группы, как, например, Korn,[158][159] Limp Bizkit[160][161] и P.O.D.,[162][163] стабильно участвуют в телепередаче Total Request Live на MTV.',\n",
              " 'question': 'Сколько в первую неделю было продано копий альбома Issues группы Korn',\n",
              " 'answers': {'text': ['Четвёртый альбом Issues (1999) группы Korn также попал на вершину Billboard 200.[118][153] За один месяц он стал трижды платиновым.[154] За первую неделю он разошёлся тиражом 573000 копий,[153] а первый сингл Falling Away from Me достиг 99 позиции в Billboard Hot 100.[155] Немногим раньше, до выхода альбома, Korn появляются в эпизоде Отличная загадка группы Korn о пиратском призраке мультсериала Южный парк, в котором состоялась премьера сингла Falling Away from Me .[156][157] В конце 1990-х и начале 2000-х различные ню-метал-группы, как, например, Korn,[158][159] Limp Bizkit[160][161] и P.O.D.,[162][163] стабильно участвуют в телепередаче Total Request Live на MTV.'],\n",
              "  'answer_start': [-1]}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'] = dataset['train'].filter(lambda x: x['id'] != 61603)"
      ],
      "metadata": {
        "id": "zGCHLHaEftea"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(dataset):\n",
        "    dataset_dict = {'texts':[], 'answers':[], 'questions':[], 'starts':[], 'ends':[]}\n",
        "\n",
        "    for item in dataset:\n",
        "        text = item['context']\n",
        "        answer = item['answers']['text'][0]\n",
        "        start = item['answers']['answer_start'][0]\n",
        "        end = start + len(answer)\n",
        "\n",
        "        # иногда индекс начала сильно неправильный или равен -1\n",
        "        # также в ответе бывают начала с большой буквы, не как в тексте\n",
        "        # или в ответе не совпадают последние (первые) знаки препинания\n",
        "        answer = answer.strip('.,?() ')\n",
        "        start = text.lower().find(answer.lower())\n",
        "        end = start + len(answer)\n",
        "        answer = text[start:end]\n",
        "\n",
        "        dataset_dict['texts'].append(text)\n",
        "        dataset_dict['answers'].append(answer)\n",
        "        dataset_dict['questions'].append(item['question'])\n",
        "        dataset_dict['starts'].append(start)\n",
        "        dataset_dict['ends'].append(end)\n",
        "\n",
        "    return dataset_dict"
      ],
      "metadata": {
        "id": "zy_fmHUoUSn_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поздно вспомнил, что можно сразу было взять срез у датасета, но пришлось бы переписывать код"
      ],
      "metadata": {
        "id": "C1RaVLq9ovto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = get_dataset(dataset['train'])\n",
        "val_dataset = get_dataset(dataset['validation'])"
      ],
      "metadata": {
        "id": "yIBXjI1OVAgG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_LEN = 3000\n",
        "VAL_LEN = 10\n",
        "\n",
        "train_dataset['texts'] = train_dataset['texts'][:TRAIN_LEN]\n",
        "train_dataset['answers'] = train_dataset['answers'][:TRAIN_LEN]\n",
        "train_dataset['questions'] = train_dataset['questions'][:TRAIN_LEN]\n",
        "train_dataset['starts'] = train_dataset['starts'][:TRAIN_LEN]\n",
        "train_dataset['ends'] = train_dataset['ends'][:TRAIN_LEN]\n",
        "\n",
        "val_dataset['texts'] = val_dataset['texts'][-VAL_LEN:]\n",
        "val_dataset['answers'] = val_dataset['answers'][-VAL_LEN:]\n",
        "val_dataset['questions'] = val_dataset['questions'][-VAL_LEN:]\n",
        "val_dataset['starts'] = val_dataset['starts'][-VAL_LEN:]\n",
        "val_dataset['ends'] = val_dataset['ends'][-VAL_LEN:]"
      ],
      "metadata": {
        "id": "8xUzv1YFk2ny"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_dict = train_dataset\n",
        "val_dataset_dict = val_dataset"
      ],
      "metadata": {
        "id": "rzx1rz3OwmWJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дообучение модели"
      ],
      "metadata": {
        "id": "TcCHhZgwvuhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = 'kisa-misa/distilrubert-tiny-cased-conversational'\n",
        "model_name = 'GeorgeKhlestov/distilbert_finetuned'\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "71cVgTnFvxyU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считал сначала также, как мы раньше делали, но без treshold.\n",
        "\n",
        "Длина была ~1800 и возникала ошибка при обучении в строке 'output = ...'\n",
        "\n",
        "RuntimeError: The size of tensor a (608) must match the size of tensor b (512) at non-singleton dimension 1\n",
        "\n",
        "Не смог понять в чем дело..."
      ],
      "metadata": {
        "id": "EYwEf1ZQRTuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 400"
      ],
      "metadata": {
        "id": "jJzKgZhH_d-j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.model_max_length = max_len"
      ],
      "metadata": {
        "id": "ZAftT4EKC3-h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_dataset_dict['texts'],  train_dataset_dict['questions'], truncation=True, padding=True, max_length=max_len)\n",
        "val_encodings = tokenizer(val_dataset_dict['texts'], val_dataset_dict['questions'], truncation=True, padding=True, max_length=max_len)"
      ],
      "metadata": {
        "id": "nZQudsUJpnhQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_positions(encodings, dataset_dict):\n",
        "\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(dataset_dict['starts'])):\n",
        "      start_positions.append(encodings.char_to_token(i, dataset_dict['starts'][i]))\n",
        "      end_positions.append(encodings.char_to_token(i, dataset_dict['ends'][i]))\n",
        "      # if None, the answer passage has been truncated\n",
        "      if start_positions[-1] is None:\n",
        "          start_positions[-1] = tokenizer.model_max_length\n",
        "      if end_positions[-1] is None:\n",
        "          end_positions[-1] = tokenizer.model_max_length\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_dataset_dict)\n",
        "add_token_positions(val_encodings, val_dataset_dict)"
      ],
      "metadata": {
        "id": "fPT4XvaZrGWU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SberquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SberquadDataset(train_encodings)\n",
        "val_dataset = SberquadDataset(val_encodings)"
      ],
      "metadata": {
        "id": "Xww8hYUer5i2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 7\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    losses = []\n",
        "    print(\"Epoch %s of %s\" %(epoch + 1, NUM_EPOCHS))\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        i += 1\n",
        "    if i == 2: break\n",
        "    print(np.array(losses).mean())\n",
        "# model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNjisKdSts4v",
        "outputId": "5ad6bd1f-45ec-415f-f601-0d5572e7bd27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 7\n",
            "2.075613071309759\n",
            "Epoch 2 of 7\n",
            "0.8448113767390556\n",
            "Epoch 3 of 7\n",
            "0.5282733193103303\n",
            "Epoch 4 of 7\n",
            "0.33897083379486775\n",
            "Epoch 5 of 7\n",
            "0.26377144019971505\n",
            "Epoch 6 of 7\n",
            "0.17572759591201517\n",
            "Epoch 7 of 7\n",
            "0.13446874211126186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "for batch in valid_loader:\n",
        "  with torch.no_grad():\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)"
      ],
      "metadata": {
        "id": "OHFqAOKNGZLf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(val_dataset)):\n",
        "    print('answer: ', val_dataset_dict['answers'][i])\n",
        "\n",
        "    start = outputs['start_logits'][i].argmax()\n",
        "    end = outputs['end_logits'][i].argmax()\n",
        "    predict_answer_tokens = batch['input_ids'][i, start:end+1]\n",
        "    print('predict:', tokenizer.decode(predict_answer_tokens), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiL7mK4QHFjz",
        "outputId": "45db31c9-dc4f-4b6a-c277-76ae703abaed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "answer:  653—661\n",
            "predict: Джим Бойд, \n",
            "\n",
            "answer:  весной 1963\n",
            "predict: 653 [UNK] 661 ) \n",
            "\n",
            "answer:  усадьбы Эусеби Гуэля\n",
            "predict: Low, \n",
            "\n",
            "answer:  Джим Бойд\n",
            "predict: смесь перхлората калия, антрацена и серы ) \n",
            "\n",
            "answer:  смесь бертолетовой соли, лактозы и канифоли\n",
            "predict: в рубиновом лазере. \n",
            "\n",
            "answer:  в рубиновом лазере\n",
            "predict: Lehman Brothers. \n",
            "\n",
            "answer:  Lehman Brothers\n",
            "predict: Light Crust Doughboys, \n",
            "\n",
            "answer:  Low\n",
            "predict: 1963 они были хорошо известны только в Ливерпуле, то в октябре того же года о них знала вся страна, \n",
            "\n",
            "answer:  Light Crust Doughboys\n",
            "predict: в III веке до нашей эры. \n",
            "\n",
            "answer:  в III веке до нашей эры\n",
            "predict: Эусеби Гуэля. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель плохо предсказывает ответы. Видно, что некоторые ее предсказания заучены от других ответов, хотя выборки я не смешивал. Видимо в них присутствуют похожие вопросы и ответы."
      ],
      "metadata": {
        "id": "eIC59OVeMwQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пути решения, которые я вижу:\n",
        "\n",
        "Увеличить тренировочные данные\n",
        "\n",
        "Возможно проблема в самом датасете, в моих изменениях в нем\n",
        "\n",
        "Еще из-за max_len=400 могли тоже случиться проблемы"
      ],
      "metadata": {
        "id": "lZO2zgm-Noqq"
      }
    }
  ]
}